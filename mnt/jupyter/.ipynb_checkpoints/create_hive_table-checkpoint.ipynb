from pyspark.sql import SparkSession

# Создать Spark сессию с поддержкой Hive
spark = SparkSession.builder \
    .master("spark://spark-master:7077") \
    .appName("CreateHiveTableApp") \
    .config("spark.hadoop.hive.metastore.uris", "thrift://hive-metastore:9083") \
    .enableHiveSupport() \
    .getOrCreate()

# Создать тестовые данные
data = [
    (1, "Alice", 25),
    (2, "Bob", 30),
    (3, "Charlie", 35)
]

df = spark.createDataFrame(data, ["id", "name", "age"])

# Показать данные
print("Исходные данные:")
df.show()

# Имя базы данных и таблицы
db_name = "default"
table_name = "test_people_table"

# Записать как управляемую (managed) таблицу Hive
# saveAsTable автоматически создает таблицу в Hive Metastore и кладет данные в spark.sql.warehouse.dir (или путь по умолчанию в HDFS)
df.write \
    .mode("overwrite") \
    .saveAsTable(f"{db_name}.{table_name}")

print(f"✓ Таблица {db_name}.{table_name} успешно создана.")

# Проверка: читаем из таблицы через SQL
print("Чтение из созданной таблицы через Spark SQL:")
spark.sql(f"SELECT * FROM {db_name}.{table_name}").show()

# spark.stop()
