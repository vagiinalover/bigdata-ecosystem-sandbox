{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "scala-header",
            "metadata": {},
            "source": [
                "# Генерация данных на Scala с Custom OutputFormat\n",
                "\n",
                "Этот ноутбук демонстрирует, как использовать Spark Scala API для подключения кастомного Hadoop OutputFormat.\n",
                "\n",
                "**Важно:** Использование кастомных OutputFormat через `saveAsNewAPIHadoopFile` отключает оптимизации Spark SQL, поэтому это работает медленнее, чем нативный `write.parquet`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "scala-init",
            "metadata": {},
            "outputs": [],
            "source": [
                "import org.apache.spark.sql.SparkSession\n",
                "import org.apache.spark.sql.functions._\n",
                "import org.apache.hadoop.mapreduce.Job\n",
                "import org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport\n",
                "import org.apache.parquet.hadoop.ParquetOutputFormat\n",
                "\n",
                "// Подключаем наш JAR (если он еще не в classpath, но обычно для ядра Toree нужно добавлять через AddJar или config при старте)\n",
                "// В данном случае мы полагаем, что JAR добавлен или лежит доступно.\n",
                "\n",
                "val spark = SparkSession.builder\n",
                "    .appName(\"ScalaLargeDataGen\")\n",
                "    .master(\"spark://spark-master:7077\")\n",
                "    .config(\"spark.hadoop.hive.metastore.uris\", \"thrift://hive-metastore:9083\")\n",
                "    .config(\"spark.jars\", \"/home/jovyan/work/custom-formats.jar\")\n",
                "    .enableHiveSupport()\n",
                "    .getOrCreate()\n",
                "\n",
                "println(\"Session Created\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "scala-data",
            "metadata": {},
            "outputs": [],
            "source": [
                "// Генерация данных\n",
                "val numRows = 10000000L\n",
                "val df = spark.range(numRows).withColumnRenamed(\"id\", \"row_id\")\n",
                "    .withColumn(\"val_int_1\", (rand() * 100).cast(\"int\"))\n",
                "    .withColumn(\"val_int_2\", (rand() * 10000).cast(\"int\"))\n",
                "    .withColumn(\"val_double_1\", rand())\n",
                "    .withColumn(\"val_double_2\", rand() * 1000.0)\n",
                "    .withColumn(\"category_code\", (rand() * 5).cast(\"int\").cast(\"string\"))\n",
                "    .withColumn(\"status\", expr(\"elt(cast(rand()*3 as int) + 1, 'ACTIVE', 'INACTIVE', 'PENDING')\"))\n",
                "    .withColumn(\"random_string_1\", hex(expr(\"rand() * 100000\").cast(\"long\")))\n",
                "    .withColumn(\"random_string_2\", concat(lit(\"prefix_\"), col(\"row_id\")))\n",
                "    .withColumn(\"created_date\", expr(\"date_add(current_date(), -cast(rand()*365 as int))\"))\n",
                "    .withColumn(\"description\", lit(\"Scala test record\"))\n",
                "\n",
                "df.printSchema()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "scala-write",
            "metadata": {},
            "outputs": [],
            "source": [
                "// Запись через RDD + Custom OutputFormat\n",
                "\n",
                "import com.example.bigdata.SizeBasedParquetOutputFormat\n",
                "import org.apache.spark.sql.Row\n",
                "\n",
                "val outputPath = \"/user/hive/warehouse/scala_custom_table_path\"\n",
                "\n",
                "// Конфигурируем Job для Parquet\n",
                "val job = Job.getInstance(spark.sparkContext.hadoopConfiguration)\n",
                "ParquetOutputFormat.setWriteSupportClass(job, classOf[ParquetWriteSupport])\n",
                "ParquetWriteSupport.setSchema(job, org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter.convert(df.schema))\n",
                "\n",
                "// Важно: RDD должен быть rddOf[(Void, Row)]\n",
                "val rdd = df.rdd.map(row => (null, row))\n",
                "\n",
                "println(s\"Writing to $outputPath using SizeBasedParquetOutputFormat...\")\n",
                "\n",
                "// Custom Format сохраняет данные\n",
                "rdd.saveAsNewAPIHadoopFile(\n",
                "    outputPath,\n",
                "    classOf[Void],\n",
                "    classOf[Row],\n",
                "    classOf[SizeBasedParquetOutputFormat],\n",
                "    job.getConfiguration\n",
                ")\n",
                "\n",
                "println(\"Write complete.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "scala-table",
            "metadata": {},
            "outputs": [],
            "source": [
                "// Регистрация Hive таблицы поверх записанных данных\n",
                "val tableName = \"scala_custom_table\"\n",
                "spark.sql(s\"DROP TABLE IF EXISTS $tableName\")\n",
                "spark.sql(s\"CREATE EXTERNAL TABLE $tableName (row_id LONG, val_int_1 INT) STORED AS PARQUET LOCATION '$outputPath'\")\n",
                "// Примечание: Для полной схемы нужно перечислить все поля в CREATE TABLE, здесь сокращено для примера\n",
                "\n",
                "spark.sql(s\"SELECT count(*) FROM $tableName\").show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Scala",
            "language": "scala",
            "name": "scala_scala"
        },
        "language_info": {
            "codemirror_mode": "text/x-scala",
            "file_extension": ".scala",
            "mimetype": "text/x-scala",
            "name": "scala",
            "pygments_lexer": "scala",
            "version": "2.12.18"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}