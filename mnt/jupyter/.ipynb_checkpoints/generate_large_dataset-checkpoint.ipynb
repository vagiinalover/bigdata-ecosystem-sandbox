{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Генерация больших данных для нагрузочного тестирования Hadoop\n",
                "\n",
                "Этот ноутбук генерирует синтетический набор данных заданного размера (1 млн, 10 млн, 100 млн строк) и сохраняет его как таблицу Hive.\n",
                "\n",
                "**Параметры:**\n",
                "- `NUM_ROWS`: Количество строк для генерации.\n",
                "- `TABLE_NAME`: Имя создаваемой таблицы."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyspark.sql import SparkSession\n",
                "from pyspark.sql.functions import col, lit, rand, concat, expr, hex\n",
                "import time\n",
                "\n",
                "# --- ПАРАМЕТРЫ КОНФИГУРАЦИИ ---\n",
                "NUM_ROWS = 1_000_000  # Измените это число: 1_000_000, 10_000_000, 100_000_000\n",
                "TABLE_NAME = \"large_scale_table\"\n",
                "# ------------------------------\n",
                "\n",
                "print(f\"Конфигурация: {NUM_ROWS} строк, таблица '{TABLE_NAME}'\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Инициализация Spark с поддержкой Hive\n",
                "spark = SparkSession.builder \\\n",
                "    .master(\"spark://spark-master:7077\") \\\n",
                "    .appName(\"LargeScaleDataGen\") \\\n",
                "    .config(\"spark.hadoop.hive.metastore.uris\", \"thrift://hive-metastore:9083\") \\\n",
                "    .enableHiveSupport() \\\n",
                "    .getOrCreate()\n",
                "\n",
                "print(\"Spark Session создана.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Генерация данных\n",
                "# Используем range для генерации ID, а затем добавляем случайные колонки\n",
                "# Это выполняется лениво и распределенно, что позволяет генерировать огромные объемы\n",
                "\n",
                "print(\"Начало генерации DataFrame...\")\n",
                "\n",
                "df = spark.range(0, NUM_ROWS).withColumnRenamed(\"id\", \"row_id\")\n",
                "\n",
                "# Добавляем 10 различных столбцов (числа и строки)\n",
                "df_with_data = df \\\n",
                "    .withColumn(\"val_int_1\", (rand() * 100).cast(\"int\")) \\\n",
                "    .withColumn(\"val_int_2\", (rand() * 10000).cast(\"int\")) \\\n",
                "    .withColumn(\"val_double_1\", rand()) \\\n",
                "    .withColumn(\"val_double_2\", rand() * 1000.0) \\\n",
                "    .withColumn(\"category_code\", (rand() * 5).cast(\"int\").cast(\"string\")) \\\n",
                "    .withColumn(\"status\", expr(\"elt(cast(rand()*3 as int) + 1, 'ACTIVE', 'INACTIVE', 'PENDING')\")) \\\n",
                "    .withColumn(\"random_string_1\", hex(expr(\"rand() * 100000\").cast(\"long\"))) \\\n",
                "    .withColumn(\"random_string_2\", concat(lit(\"prefix_\"), col(\"row_id\"))) \\\n",
                "    .withColumn(\"created_date\", expr(\"date_add(current_date(), -cast(rand()*365 as int))\")) \\\n",
                "    .withColumn(\"description\", lit(\"This is a test record for load testing Hadoop cluster capacity\"))\n",
                "\n",
                "print(\"Схема данных:\")\n",
                "df_with_data.printSchema()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Запись данных в Hive\n",
                "start_time = time.time()\n",
                "print(f\"Начинаем запись {NUM_ROWS} строк в таблицу {TABLE_NAME}...\")\n",
                "\n",
                "# repartition может помочь создать файлы оптимального размера, \n",
                "# но для скорости генерации можно убрать или настроить (например, 10-100 партиций)\n",
                "df_with_data.write \\\n",
                "    .mode(\"overwrite\") \\\n",
                "    .format(\"parquet\") \\\n",
                "    .saveAsTable(f\"default.{TABLE_NAME}\")\n",
                "\n",
                "end_time = time.time()\n",
                "duration = end_time - start_time\n",
                "\n",
                "print(f\"✓ Готово! Записано {NUM_ROWS} строк.\")\n",
                "print(f\"Время выполнения: {duration:.2f} секунд\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Проверка результата (показать первые 5 строк)\n",
                "spark.sql(f\"SELECT * FROM default.{TABLE_NAME} LIMIT 5\").show(truncate=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Показать общее количество строк в таблице (проверка)\n",
                "count = spark.sql(f\"SELECT count(*) FROM default.{TABLE_NAME}\").collect()[0][0]\n",
                "print(f\"Количество строк в таблице: {count}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "spark.stop()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}