{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "scala-header",
   "metadata": {},
   "source": [
    "# Scala Smart Generation: Автоматический расчет размера файлов\n",
    "\n",
    "**Исправления (v5):**\n",
    "1.  **Calibration Fix**: Временные файлы пишутся в `spark.sql.warehouse.dir`, чтобы гарантировать использование той же файловой системы (HDFS/Local), что и основная таблица.\n",
    "2.  **Verification Fix**: Путь к таблице берется из метаданных каталога (`spark.sessionState.catalog`), а не конструируется вручную."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "scala-init",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session Created\n",
      "Default FS: file:///\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@278de205\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@278de205"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.{SparkSession, DataFrame}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.hadoop.fs.{FileSystem, Path}\n",
    "\n",
    "val spark = SparkSession.builder\n",
    "    .appName(\"ScalaSmartScale_v2\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .config(\"spark.hadoop.hive.metastore.uris\", \"thrift://hive-metastore:9083\")\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    "\n",
    "println(\"Spark Session Created\")\n",
    "println(\"Default FS: \" + spark.sparkContext.hadoopConfiguration.get(\"fs.defaultFS\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "scala-calib-func",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "calculateMaxRecordsPerFile: (df: org.apache.spark.sql.DataFrame, targetSizeMB: Int, sampleFraction: Double)Int\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "/**\n",
    "  * Функция калибровки. Считает оптимальное кол-во строк на файл.\n",
    "  */\n",
    "def calculateMaxRecordsPerFile(df: DataFrame, targetSizeMB: Int = 128, sampleFraction: Double = 0.01): Int = {\n",
    "    println(\"--- Запуск калибровки ---\")\n",
    "\n",
    "    val sampleDf = df.sample(withReplacement = false, fraction = sampleFraction, seed = 42).cache()\n",
    "    val sampleCount = sampleDf.count()\n",
    "    \n",
    "    if (sampleCount == 0) return 1000000\n",
    "\n",
    "    // ИСПОЛЬЗУЕМ WAREHOUSE DIR ДЛЯ ВРЕМЕННЫХ ФАЙЛОВ\n",
    "    // Это гарантирует, что мы пишем и читаем из одной и той же файловой системы.\n",
    "    val warehouseDir = spark.conf.get(\"spark.sql.warehouse.dir\", \"/user/hive/warehouse\")\n",
    "    val tempPathString = s\"$warehouseDir/temp_calib_${java.util.UUID.randomUUID.toString}\"\n",
    "    val tempPath = new Path(tempPathString)\n",
    "    \n",
    "    println(s\"Temp calibration path: $tempPath\")\n",
    "\n",
    "    try {\n",
    "        sampleDf.write.mode(\"overwrite\").parquet(tempPathString)\n",
    "        \n",
    "        // ПОЛУЧАЕМ FS ИЗ ПУТИ\n",
    "        val fs = tempPath.getFileSystem(spark.sparkContext.hadoopConfiguration)\n",
    "        println(s\"Calibration FileSystem: ${fs.getScheme}://${fs.getUri.getAuthority}\")\n",
    "\n",
    "        val contentSummary = fs.getContentSummary(tempPath)\n",
    "        val totalBytes = contentSummary.getLength\n",
    "        \n",
    "        println(s\"Сэмпл: $sampleCount строк\")\n",
    "        println(s\"Размер на диске: ${totalBytes} байт (${totalBytes/1024/1024.0} MB)\")\n",
    "\n",
    "        // Чистим за собой\n",
    "        fs.delete(tempPath, true)\n",
    "        sampleDf.unpersist()\n",
    "\n",
    "        if (totalBytes == 0) return 1000000 // Защита от 0\n",
    "\n",
    "        val bytesPerRow = totalBytes.toDouble / sampleCount\n",
    "        val targetBytes = targetSizeMB * 1024 * 1024L\n",
    "        val estimatedRecords = (targetBytes / bytesPerRow).toInt\n",
    "        \n",
    "        println(s\"Средний размер строки: ${f\"$bytesPerRow%.2f\"} байт\")\n",
    "        println(s\"Рекомендуемый maxRecords: $estimatedRecords\")\n",
    "        println(\"-------------------------\")\n",
    "        \n",
    "        estimatedRecords\n",
    "    } catch {\n",
    "        case e: Exception =>\n",
    "            println(\"Ошибка калибровки: \" + e.getMessage)\n",
    "            e.printStackTrace()\n",
    "            1000000\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "scala-data-gen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Датасет определен: 5000000 строк\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numRows = 5000000\n",
       "df = [row_id: bigint, heavy_text_1: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[row_id: bigint, heavy_text_1: string ... 2 more fields]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numRows = 5000000L \n",
    "val df = spark.range(numRows).withColumnRenamed(\"id\", \"row_id\")\n",
    "    .withColumn(\"heavy_text_1\", concat(lit(\"desc_\"), hex(expr(\"rand()*1000000\").cast(\"long\"))))\n",
    "    .withColumn(\"heavy_text_2\", when(rand() > 0.5, lit(\"A long filler text string to simulate volume\")).otherwise(lit(null)))\n",
    "    .withColumn(\"numbers\", (rand() * 10000).cast(\"double\"))\n",
    "\n",
    "println(s\"Датасет определен: $numRows строк\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "scala-execute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Запуск калибровки ---\n",
      "Temp calibration path: file:/home/jovyan/work/spark-warehouse/temp_calib_4cd5cae6-9deb-4978-b84d-8b099bb2f3c8\n",
      "Ошибка калибровки: Job aborted due to stage failure: Task 0 in stage 4.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4.0 (TID 11) (172.18.0.8 executor 0): java.io.IOException: Mkdirs failed to create file:/home/jovyan/work/spark-warehouse/temp_calib_4cd5cae6-9deb-4978-b84d-8b099bb2f3c8/_temporary/0/_temporary/attempt_202601182301282926008902230024666_0004_m_000000_11 (exists=false, cwd=file:/opt/spark/work/app-20260118230120-0009/0)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n",
      "\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "\n",
      "Driver stacktrace:\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4.0 (TID 11) (172.18.0.8 executor 0): java.io.IOException: Mkdirs failed to create file:/home/jovyan/work/spark-warehouse/temp_calib_4cd5cae6-9deb-4978-b84d-8b099bb2f3c8/_temporary/0/_temporary/attempt_202601182301282926008902230024666_0004_m_000000_11 (exists=false, cwd=file:/opt/spark/work/app-20260118230120-0009/0)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n",
      "\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:390)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:418)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:390)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:792)\n",
      "\tat $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.calculateMaxRecordsPerFile(<console>:49)\n",
      "\tat $line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:31)\n",
      "\tat $line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:51)\n",
      "\tat $line16.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:53)\n",
      "\tat $line16.$read$$iw$$iw$$iw$$iw$$iw.<init>(<console>:55)\n",
      "\tat $line16.$read$$iw$$iw$$iw$$iw.<init>(<console>:57)\n",
      "\tat $line16.$read$$iw$$iw$$iw.<init>(<console>:59)\n",
      "\tat $line16.$read$$iw$$iw.<init>(<console>:61)\n",
      "\tat $line16.$read$$iw.<init>(<console>:63)\n",
      "\tat $line16.$read.<init>(<console>:65)\n",
      "\tat $line16.$read$.<init>(<console>:69)\n",
      "\tat $line16.$read$.<clinit>(<console>)\n",
      "\tat $line16.$eval$.$print$lzycompute(<console>:7)\n",
      "\tat $line16.$eval$.$print(<console>:6)\n",
      "\tat $line16.$eval.$print(<console>)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n",
      "\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)\n",
      "\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)\n",
      "\tat scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)\n",
      "\tat scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)\n",
      "\tat scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)\n",
      "\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)\n",
      "\tat scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)\n",
      "\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)\n",
      "\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:564)\n",
      "\tat org.apache.toree.kernel.interpreter.scala.ScalaInterpreterSpecific.$anonfun$interpretAddTask$2(ScalaInterpreterSpecific.scala:381)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat scala.Console$.withErr(Console.scala:196)\n",
      "\tat org.apache.toree.global.StreamState$.$anonfun$withStreams$2(StreamState.scala:74)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat scala.Console$.withOut(Console.scala:167)\n",
      "\tat org.apache.toree.global.StreamState$.$anonfun$withStreams$1(StreamState.scala:73)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat scala.Console$.withIn(Console.scala:230)\n",
      "\tat org.apache.toree.global.StreamState$.withStreams(StreamState.scala:72)\n",
      "\tat org.apache.toree.kernel.interpreter.scala.ScalaInterpreterSpecific.$anonfun$interpretAddTask$1(ScalaInterpreterSpecific.scala:376)\n",
      "\tat org.apache.toree.utils.TaskManager$$anon$2.run(TaskManager.scala:134)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "Caused by: java.io.IOException: Mkdirs failed to create file:/home/jovyan/work/spark-warehouse/temp_calib_4cd5cae6-9deb-4978-b84d-8b099bb2f3c8/_temporary/0/_temporary/attempt_202601182301282926008902230024666_0004_m_000000_11 (exists=false, cwd=file:/opt/spark/work/app-20260118230120-0009/0)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n",
      "\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\t... 3 more\n",
      "Записываем таблицу scala_smart_table_v2...\n",
      "Запись завершена.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "optimalMaxRecords = 1000000\n",
       "tableName = scala_smart_table_v2\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "scala_smart_table_v2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// 1. Калибровка\n",
    "val optimalMaxRecords = calculateMaxRecordsPerFile(df, targetSizeMB = 128, sampleFraction = 0.05)\n",
    "\n",
    "val tableName = \"scala_smart_table_v2\"\n",
    "\n",
    "// 2. Запись\n",
    "println(s\"Записываем таблицу $tableName...\")\n",
    "df.write\n",
    "  .mode(\"overwrite\")\n",
    "  .option(\"maxRecordsPerFile\", optimalMaxRecords)\n",
    "  .saveAsTable(tableName)\n",
    "println(\"Запись завершена.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "scala-check",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table Location: hdfs://namenode:9000/user/hive/warehouse/scala_smart_table_v2\n",
      "Файлы:\n",
      "File: part-00000-6684cc02-05eb-49e2-a943-eb7577246df4-c000.snappy.parquet, Size: 17.69 MB\n",
      "File: part-00000-6684cc02-05eb-49e2-a943-eb7577246df4-c001.snappy.parquet, Size: 17.69 MB\n",
      "File: part-00000-6684cc02-05eb-49e2-a943-eb7577246df4-c002.snappy.parquet, Size: 8.85 MB\n",
      "File: part-00001-6684cc02-05eb-49e2-a943-eb7577246df4-c000.snappy.parquet, Size: 17.69 MB\n",
      "File: part-00001-6684cc02-05eb-49e2-a943-eb7577246df4-c001.snappy.parquet, Size: 17.69 MB\n",
      "File: part-00001-6684cc02-05eb-49e2-a943-eb7577246df4-c002.snappy.parquet, Size: 8.85 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tableMetadata = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "CatalogTable(\n",
       "Catalog: spark_catalog\n",
       "Database: default\n",
       "Table: scala_smart_table_v2\n",
       "Owner: root\n",
       "Created Time: Sun Jan 18 23:01:32 UTC 2026\n",
       "Last Access: UNKNOWN\n",
       "Created By: Spark 3.5.1\n",
       "Type: MANAGED\n",
       "Provider: parquet\n",
       "Statistics: 92768759 bytes\n",
       "Location: hdfs://namenode:9000/user/hive/warehouse/scala_smart_table_v2\n",
       "Serde Library: org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\n",
       "InputFormat: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\n",
       "OutputFormat: org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\n",
       "Storage Properties: [maxRecordsPerFile=1000000]\n",
       "Schema: root\n",
       " |-- row_id: long (nullable = true)\n",
       " |-- heavy_text_1: string (nullable = ...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "// 3. Проверка результата\n",
    "import org.apache.spark.sql.catalyst.TableIdentifier\n",
    "\n",
    "// Правильный способ узнать путь к таблице из метастора\n",
    "val tableMetadata = spark.sessionState.catalog.getTableMetadata(TableIdentifier(tableName))\n",
    "val tableUri = tableMetadata.location\n",
    "val tablePath = new Path(tableUri)\n",
    "\n",
    "// Получаем правильную FS для этого пути\n",
    "val fs = tablePath.getFileSystem(spark.sparkContext.hadoopConfiguration)\n",
    "\n",
    "println(s\"Table Location: $tableUri\")\n",
    "\n",
    "println(\"Файлы:\")\n",
    "fs.listStatus(tablePath).foreach { status =>\n",
    "    if (status.isFile && status.getPath.getName.startsWith(\"part\")) {\n",
    "        val sizeMB = status.getLen / 1024.0 / 1024.0\n",
    "        println(s\"File: ${status.getPath.getName}, Size: ${f\"$sizeMB%.2f\"} MB\")\n",
    "    }\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scala - Scala",
   "language": "scala",
   "name": "scala_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.12.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
