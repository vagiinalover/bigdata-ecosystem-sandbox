{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "652349f1",
   "metadata": {},
   "source": [
    "# Генерация больших данных для нагрузочного тестирования Hadoop\n",
    "\n",
    "Этот ноутбук генерирует синтетический набор данных заданного размера и сохраняет его как таблицу Hive.\n",
    "\n",
    "**Способ избежания OOM и больших файлов:**\n",
    "- Мы используем параметр конфигурации `spark.sql.files.maxRecordsPerFile`.\n",
    "- Это заставляет Spark автоматически \"нарезать\" выходные файлы по количеству строк.\n",
    "- Мы ставим значение **3 000 000 строк**, что для текущей структуры данных примерно соответствует 100-120 МБ.\n",
    "\n",
    "**Custom OutputFormat:**\n",
    "- Мы также загружаем скомпилированный JAR (`custom-formats.jar`) с вашим классом `com.example.bigdata.SizeBasedParquetOutputFormat`. Он доступен в classpath, если вы захотите использовать его через RDD API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc43110",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, rand, concat, expr, hex\n",
    "import time\n",
    "import os\n",
    "\n",
    "# --- ПАРАМЕТРЫ КОНФИГУРАЦИИ ---\n",
    "NUM_ROWS = 100_000_000  # Желаемое количество строк\n",
    "TABLE_NAME = \"large_scale_table\"\n",
    "MAX_RECORDS_PER_FILE = 3_000_000 # ~120 MB на файл\n",
    "CUSTOM_JAR_PATH = \"/home/jovyan/work/custom-formats.jar\"\n",
    "# ------------------------------\n",
    "\n",
    "print(f\"Конфигурация: {NUM_ROWS} строк, таблица '{TABLE_NAME}'\")\n",
    "if os.path.exists(CUSTOM_JAR_PATH):\n",
    "    print(f\"Custom JAR найден: {CUSTOM_JAR_PATH}\")\n",
    "else:\n",
    "    print(\"Custom JAR не найден! Сначала запустите компиляцию.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1251738",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .appName(\"LargeScaleDataGen\") \\\n",
    "    .config(\"spark.hadoop.hive.metastore.uris\", \"thrift://hive-metastore:9083\") \\\n",
    "    .config(\"spark.jars\", CUSTOM_JAR_PATH) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .config(\"spark.sql.files.maxRecordsPerFile\", str(MAX_RECORDS_PER_FILE)) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Session создана (Custom JAR загружен).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a92cd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерация данных\n",
    "print(\"Формирование DataFrame...\")\n",
    "\n",
    "df = spark.range(0, NUM_ROWS).withColumnRenamed(\"id\", \"row_id\")\n",
    "\n",
    "df_with_data = df \\\n",
    "    .withColumn(\"val_int_1\", (rand() * 100).cast(\"int\")) \\\n",
    "    .withColumn(\"val_int_2\", (rand() * 10000).cast(\"int\")) \\\n",
    "    .withColumn(\"val_double_1\", rand()) \\\n",
    "    .withColumn(\"val_double_2\", rand() * 1000.0) \\\n",
    "    .withColumn(\"category_code\", (rand() * 5).cast(\"int\").cast(\"string\")) \\\n",
    "    .withColumn(\"status\", expr(\"elt(cast(rand()*3 as int) + 1, 'ACTIVE', 'INACTIVE', 'PENDING')\")) \\\n",
    "    .withColumn(\"random_string_1\", hex(expr(\"rand() * 100000\").cast(\"long\"))) \\\n",
    "    .withColumn(\"random_string_2\", concat(lit(\"prefix_\"), col(\"row_id\"))) \\\n",
    "    .withColumn(\"created_date\", expr(\"date_add(current_date(), -cast(rand()*365 as int))\")) \\\n",
    "    .withColumn(\"description\", lit(\"This is a test record for load testing Hadoop cluster capacity\"))\n",
    "\n",
    "print(\"Схема:\")\n",
    "df_with_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1d75f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(f\"Начинаем запись данных в таблицу {TABLE_NAME}...\")\n",
    "\n",
    "# Используем стандартный writer с параметром maxRecordsPerFile\n",
    "# Это самый стабильный способ контролировать размер файлов.\n",
    "\n",
    "df_with_data \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .saveAsTable(f\"default.{TABLE_NAME}\")\n",
    "\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "\n",
    "print(f\"✓ Готово! Записано {NUM_ROWS} строк.\")\n",
    "print(f\"Время выполнения: {duration:.2f} секунд\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe03c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверка результата (имен файлов)\n",
    "spark.sql(f\"SELECT * FROM default.{TABLE_NAME} LIMIT 5\").show(truncate=False)\n",
    "\n",
    "count = spark.sql(f\"SELECT count(*) FROM default.{TABLE_NAME}\").collect()[0][0]\n",
    "print(f\"Проверка количества строк: {count}\")\n",
    "\n",
    "# Показать количество созданных файлов через hadoop input_file_name()\n",
    "from pyspark.sql.functions import input_file_name\n",
    "file_count = spark.table(f\"default.{TABLE_NAME}\").select(input_file_name()).distinct().count()\n",
    "print(f\"Количество созданных файлов: {file_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048e7c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
