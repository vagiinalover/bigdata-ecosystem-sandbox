{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "626ea432-f6e0-468f-812e-23dd0effe40c",
   "metadata": {},
   "source": [
    "# Генерация больших данных для нагрузочного тестирования Hadoop\n",
    "\n",
    "Этот ноутбук генерирует синтетический набор данных заданного размера и сохраняет его как таблицу Hive.\n",
    "\n",
    "**Стратегия управления размером файлов (Native Spark AQE):**\n",
    "- Мы включаем **Adaptive Query Execution (AQE)**.\n",
    "- Задаем целевой размер партиции (`spark.sql.adaptive.advisoryPartitionSizeInBytes`) в **128 МБ**.\n",
    "- Делаем `repartition` на заведомо **большое** количество частей (например, 1000).\n",
    "- **Spark сам** объединяет (coalesce) эти мелкие части в файлы по ~128 МБ во время записи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63f1c895-e4a9-476f-b031-db712b900a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Конфигурация: 1000000 строк, таблица 'large_scale_table', цель по размеру файла: 128 MB\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, rand, concat, expr, hex\n",
    "import time\n",
    "\n",
    "# --- ПАРАМЕТРЫ КОНФИГУРАЦИИ ---\n",
    "NUM_ROWS = 1_000_000  # Желаемое количество строк\n",
    "TABLE_NAME = \"large_scale_table\"\n",
    "TARGET_FILE_SIZE_B = 128 * 1024 * 1024  # 128 MB в байтах\n",
    "# ------------------------------\n",
    "\n",
    "print(f\"Конфигурация: {NUM_ROWS} строк, таблица '{TABLE_NAME}', цель по размеру файла: 128 MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ac25784-525f-4d35-ae21-fa5b7b83c51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/18 21:06:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session создана с включенным AQE.\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .master(\"spark://spark-master:7077\")\n",
    "        .appName(\"LargeScaleDataGen\")\n",
    "        .config(\"spark.hadoop.hive.metastore.uris\", \"thrift://hive-metastore:9083\")\n",
    "        .enableHiveSupport()\n",
    "\n",
    "        # --- ВКЛЮЧАЕМ AQE (Адаптивное выполнение) ---\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "        .config(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", str(TARGET_FILE_SIZE_B))\n",
    "        .config(\"spark.sql.parquet.block.size\", str(TARGET_FILE_SIZE_B))\n",
    "\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"Spark Session создана с включенным AQE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17073eee-7b60-410c-8bb3-e0ea0736d3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Формирование DataFrame...\n",
      "Схема:\n",
      "root\n",
      " |-- row_id: long (nullable = false)\n",
      " |-- val_int_1: integer (nullable = true)\n",
      " |-- val_int_2: integer (nullable = true)\n",
      " |-- val_double_1: double (nullable = false)\n",
      " |-- val_double_2: double (nullable = false)\n",
      " |-- category_code: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- random_string_1: string (nullable = true)\n",
      " |-- random_string_2: string (nullable = false)\n",
      " |-- created_date: date (nullable = true)\n",
      " |-- description: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Генерация данных\n",
    "print(\"Формирование DataFrame...\")\n",
    "\n",
    "df = spark.range(0, NUM_ROWS).withColumnRenamed(\"id\", \"row_id\")\n",
    "\n",
    "df_with_data = df \\\n",
    "    .withColumn(\"val_int_1\", (rand() * 100).cast(\"int\")) \\\n",
    "    .withColumn(\"val_int_2\", (rand() * 10000).cast(\"int\")) \\\n",
    "    .withColumn(\"val_double_1\", rand()) \\\n",
    "    .withColumn(\"val_double_2\", rand() * 1000.0) \\\n",
    "    .withColumn(\"category_code\", (rand() * 5).cast(\"int\").cast(\"string\")) \\\n",
    "    .withColumn(\"status\", expr(\"elt(cast(rand()*3 as int) + 1, 'ACTIVE', 'INACTIVE', 'PENDING')\")) \\\n",
    "    .withColumn(\"random_string_1\", hex(expr(\"rand() * 100000\").cast(\"long\"))) \\\n",
    "    .withColumn(\"random_string_2\", concat(lit(\"prefix_\"), col(\"row_id\"))) \\\n",
    "    .withColumn(\"created_date\", expr(\"date_add(current_date(), -cast(rand()*365 as int))\")) \\\n",
    "    .withColumn(\"description\", lit(\"This is a test record for load testing Hadoop cluster capacity\"))\n",
    "\n",
    "print(\"Схема:\")\n",
    "df_with_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f1f7a44-7c3b-4a89-acd0-274899ba64f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начинаем запись данных в таблицу large_scale_table...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/18 21:09:34 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Готово! Записано 1000000 строк.\n",
      "Время выполнения: 174.59 секунд\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(f\"Начинаем запись данных в таблицу {TABLE_NAME}...\")\n",
    "\n",
    "# СТРАТЕГИЯ AQE \"OVERSHOOT\":\n",
    "# 1. Принудительно разбиваем данные на БОЛЬШОЕ количество мелких партиций.\n",
    "#    Число 1000 берем с запасом (расчетно 100 млн строк это ~3 Гб, \n",
    "#    1000 партиций дадут ~3 Мб на партицию, что намного меньше 128 Мб).\n",
    "# 2. AQE увидит, что партиции слишком маленькие, и объединит их до целевого размера (128 Мб).\n",
    "\n",
    "OVER_PARTITION_COUNT = 1000\n",
    "\n",
    "df_with_data \\\n",
    "    .repartition(OVER_PARTITION_COUNT) \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .saveAsTable(f\"default.{TABLE_NAME}\")\n",
    "\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "\n",
    "print(f\"✓ Готово! Записано {NUM_ROWS} строк.\")\n",
    "print(f\"Время выполнения: {duration:.2f} секунд\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c9e3ac0-ba08-4dbd-8799-7fff4c42295a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+---------+--------------------+------------------+-------------+--------+---------------+---------------+------------+--------------------------------------------------------------+\n",
      "|row_id|val_int_1|val_int_2|val_double_1        |val_double_2      |category_code|status  |random_string_1|random_string_2|created_date|description                                                   |\n",
      "+------+---------+---------+--------------------+------------------+-------------+--------+---------------+---------------+------------+--------------------------------------------------------------+\n",
      "|200644|47       |994      |0.024814132855865978|230.371993448805  |1            |INACTIVE|11C2D          |prefix_200644  |2025-12-10  |This is a test record for load testing Hadoop cluster capacity|\n",
      "|497917|56       |6162     |0.12605512751903447 |714.1828741755163 |4            |PENDING |2F0C           |prefix_497917  |2025-08-19  |This is a test record for load testing Hadoop cluster capacity|\n",
      "|492325|90       |5982     |0.35829224723560005 |31.56588688767481 |3            |PENDING |13915          |prefix_492325  |2025-03-08  |This is a test record for load testing Hadoop cluster capacity|\n",
      "|476710|63       |2361     |0.2984071018270086  |150.38734941529353|1            |INACTIVE|F995           |prefix_476710  |2025-10-08  |This is a test record for load testing Hadoop cluster capacity|\n",
      "|234266|88       |9686     |0.18192775189412735 |600.7167813934652 |1            |INACTIVE|12594          |prefix_234266  |2026-01-03  |This is a test record for load testing Hadoop cluster capacity|\n",
      "+------+---------+---------+--------------------+------------------+-------------+--------+---------------+---------------+------------+--------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:=================================================>       (28 + 2) / 32]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Проверка количества строк: 1000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Проверка результата\n",
    "spark.sql(f\"SELECT * FROM default.{TABLE_NAME} LIMIT 5\").show(truncate=False)\n",
    "\n",
    "count = spark.sql(f\"SELECT count(*) FROM default.{TABLE_NAME}\").collect()[0][0]\n",
    "print(f\"Проверка количества строк: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f54cfcfd-e3af-45e4-ad39-cbe2d826a163",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9969dcc4-ae80-4916-9576-a6bb89afbc0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
